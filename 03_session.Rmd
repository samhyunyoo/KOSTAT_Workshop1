---
title: "Session 3 notes"
author: "Tim Riffe"
date: "2022-07-29"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Review of tidyverse data processing function

`filter()` is for rows, you need to know logical expressions
`select()` is for columns, there are helper functions, like `contains()` or you can give ranges.
`group_by()` whenever you want to do things separately in your data, don't forget to `ungroup()` when you're done!
`mutate()` for creating or modifying columns
`summarize()` is for making aggregations of various kinds, usually done together with `group_by()`.

### we didn't yet cover:
`pivot_longer()` for gathering columns and stacking them.
`pivot_wider()` for spreading out the contents of a column over a new range of columns.

## Get the data

We went to github, clicked on the file name once, then clicked `Download` and put it in the `Data` folder.

## Read in the data

We use the `readxl` package to exract tables from spreadsheets.
```{r, message = FALSE, warning = FALSE}
library(tidyverse)
# install.package("readxl")
library(readxl)
```

```{r}
B <- read_excel(path = "Data/demo_fasec.xlsx",
                range = "A10:H158",
                na = ":")
# View(B)
# ?read_excel
```

## Step 1 reshape to long

Pivot wider is for stacking columns. It usually results in fewer columns and always results in more rows. In our case, we're collecting 6 columns, each with 148 values, so that resulting data object should have `148 * 6 = 888` rows. And we want TWO new columns, births and year. We create these names using `names_to` and `values_to`. There are flexible ways of specifying which columns to collect.

```{r}
B <-
  B %>% 
  pivot_longer(cols = `2011`:`2016`,
               # cols = 3:8
               # cols = 3:ncol(.)
               names_to = "year",
               values_to = "births")
```

### Time out for `pivot_wider()`

Usage of `pivot_wider()` is a bit simpler: you just need to tell it which columns to spread out. One becomes the names (should be limited categories) (`names_from`), and the other is what to put in the cells `values_from`.
```{r}
B %>% 
  pivot_wider(names_from = "year",
              values_from = "births")
```

## Step 2 simplify column names

We could use `select()`, but you can't leave any columns out.
Instead we use `rename()` because it requires less typing, and does just this one thing. To be explicit we rename using `to = from`.
```{r}
# B %>% 
#   select(age = AGE,
#          country = `GEO/TIME`,
#          year,
#          births)
B <-
  B %>% 
  rename(age = AGE,
         country =`GEO/TIME`)
```

## Step 3 rescale births

We usually respect stated totals by rescaling observed distributions to be constrained to the stated total. We do this knowing that possibly the age (or whatever) distribution of births of unknown maternal age might not be the same. So this operation might induce bias if it is both the case that the fraction of births in unknown ages is high and the distributions is different.

$$  \hat{B_x} = \frac{B_x}{\sum{B_x}} \cdot T $$

```{r}
B <- 
  B %>% 
  # filter(country == "Belgium",
  #        year == 2015) %>% 
  group_by(country, year) %>% 
  mutate(total = births[age == "Total"]) %>% 
  filter(! age %in% c("Total", "Unknown")) %>% 
  mutate(#dist = births / sum(births),
         #births = dist * total
         births_hat = births / sum(births) * total)

# quick check: are we being overly manipulative here or not?
# conclusion: no, worst case is 1% unknown.
B %>% 
  group_by(country, year) %>% 
  summarize(total = births[age == "Total"],
            unk = births[age == "Unknown"]) %>% 
  ungroup() %>% 
  mutate(badness = unk / total * 100)
# births / sum(births) * total
```

## Step 4 convert age to integer

I mentioned some other ways we could pick out the age numbers from the strings. The function `parse_number()`, which loaded with the tidyverse is usually the cheapest option.
```{r}
B %>% pull(age) %>% unique()
B <-
  B %>% 
  mutate(age = parse_number(age))
```

## put it all together

```{r}
B <-
  # 1. read in from range we got from visual inspection,
  # explicitly specify the NA character
  read_excel(path = "Data/demo_fasec.xlsx",
             range = "A10:H158",
             na = ":") %>% 
  
  # 2. then stack the columns for births by year,
  # columns "2011" until "2016" are removed, 
  # and year and births are created
  pivot_longer(cols = `2011`:`2016`,
               names_to = "year",
               values_to = "births") %>% 
  
  # 3. clean the names
  rename(age = AGE,
         country =`GEO/TIME`) %>% 
  
  # 4. rescale to totals within unique subsets of year and country
  group_by(country, year) %>% 
  
  # bring out total to a new column
  mutate(total = births[age == "Total"]) %>% 
  
  # throw out these rows, otherwise they contaminate the distribution
  filter(! age %in% c("Total", "Unknown")) %>% 
  
  # we assume unknowns follow same distribution as knowns
  mutate(births_hat = births / sum(births) * total) %>% 
  ungroup() %>% 
  
  # 5. clean age column
  mutate(age = parse_number(age))
  
```



## Now let's get exposures


### 1. read in population

This works just as before
```{r}
P <- 
  read_excel(path = "Data/demo_pjan.xlsx",
             range = "A10:CZ510",
             na = ":")

```

### 2. `pivot_longer()` for age

reshape to longer, creating new columns for age and pop.
```{r}
P <-
  P %>% 
  pivot_longer(#cols = 3:104,
               `Less than 1 year`:Unknown,
               names_to = "age",
               values_to = "pop")
# P %>% colnames()
```

### 3. remove the NA rows

We also do some checks in case some subsets have unexpected missings
```{r}
P <- 
  P %>% 
  filter(!is.na(pop))
# check subset sizes; some close out at 85+ or 90+
# P %>% 
#   group_by(TIME, `GEO/AGE`) %>% 
#   summarize(n = n()) %>% 
#   View()
# 
# P %>% 
#   filter(`GEO/AGE` == "Albania",
#          TIME == 2012) %>% 
#   View()
```

### 4. rename columns where helpful

```{r}
P <- 
  P %>% 
  rename(year = TIME,
         country = `GEO/AGE`)
```

### 5. redisitribute unknowns

$$ \hat{P_x} = P_x + \frac{P_x}{\sum P_x} * P_{Unk} $$

```{r}
P <- 
  P %>% 
  group_by(country, year) %>% 
  mutate(unk = pop[age == "Unknown"]) %>% 
  filter(age != "Unknown") %>% 
  mutate(pop_hat = pop + pop / sum(pop) * unk) %>% 
  ungroup() 

  # eyeball check to make sure we actually did something:
  # filter(country == "North Macedonia") %>% pull(pop_hat)

  # just checking in case we need to be more rigourous,
  # do we have missing unknown ages? NO
  # ungroup() %>% 
  # filter(is.na(unk))
```

### 6. recode age


Since we have a few cases, we'll use `case_when()`, which is constructed like so, moving from specific to general cases because they are evaluated in order.

```{r, eval = FALSE}
case_when(condition_1 ~ case_1,
          condition_2 ~ case_2,
          condition_3 ~ case_3,
          TRUE ~ everything_else)
```

Here used for our age classes:
```{r, warning = FALSE}
# handle cases:
# P %>% pull(age) %>% unique()
P <- 
  P %>% 
  mutate(age = 
           case_when(age == "Less than 1 year" ~ 0,
                     age == "Open-ended age class" ~ 100,
                     TRUE ~ parse_number(age)))
```


### 7. calculate exposure
```{r}
a <- 0:10
a
lead(a)
lag(a)
```

```{r}
P <-
  P %>% 
  arrange(country, age, year) %>% 
  group_by(country, age) %>% 
  mutate(pop2 = lead(pop_hat)) %>% 
  ungroup() %>% 
  filter(!is.na(pop2)) %>% 
  mutate(expos = (pop2 + pop_hat) / 2)
```

### 8. bring it all together

```{r, message = FALSE}
E <- 
  read_excel(path = "Data/demo_pjan.xlsx",
             range = "A10:CZ510",
             na = ":") %>% 
  
  # stack ages: column range via visual inspection,
  # creating age and pop columns.
  pivot_longer(#cols = 3:104,
               `Less than 1 year`:Unknown,
               names_to = "age",
               values_to = "pop") %>% 
  
  # remove NAs, which are either missing years or highest ages
  # for those countries with lower closeout ages
  filter(!is.na(pop)) %>% 
  
  # clean the names 
  rename(year = TIME,
         country = `GEO/AGE`) %>% 
  
  # redistribute unknown ages
  group_by(country, year) %>% 
  mutate(unk = pop[age == "Unknown"]) %>% 
  filter(age != "Unknown") %>% 
  mutate(pop1 = pop + pop / sum(pop) * unk) %>% 
  ungroup() %>% 
  
  # recode age
  mutate(age = 
           case_when(age == "Less than 1 year" ~ 0,
                     age == "Open-ended age class" ~ 100,
                     TRUE ~ parse_number(age))) %>% 
  
  # get jan 1 and dec1 (next year's jan 1) next to each other
  # *try to understand the logical of sorting (arrange()) and
  # groups.
  arrange(country, age, year) %>% 
  group_by(country, age) %>% 
  mutate(pop2 = lead(pop1)) %>% 
  ungroup() %>% 
  
  # removes final year, for which we can't calculate exposure
  filter(!is.na(pop2)) %>% 
  
  # finally, exposure as July 1 approximation.
  # This isn't the only way to do it..
  mutate(expos = (pop2 + pop1) / 2)

```


## merge and calculate

Now we have `B` and `E`, with `country`, `year`, and `age` with identical names and codes (values). That is, we can establish matches. `B` has a year that `E` does not have. `E` has countries, years, and ages that `B` does not have. We only care about overlapping cases, so we do the strictest kind of join, and `inner_join()`.
```{r}

BE <-
  inner_join(B, 
             E, 
             # what variables do we match on?
             by = c("country", "year","age")) %>% 
  
  # select only needed columns
  select(country, year, age, births, births_hat, expos) %>% 
  
  # Age-specific fertility (fx) is events / exposure
  mutate(fx = births_hat / expos)
```

Now we are fully ready to calculate all kinds of indices. Any index in our case, will be calculated independently for each country and year, so we group by country and year. Each country-year combination will result in a row of the outgoing data. We calculate the total fertility rate (TFR), using the adjusted and unadjusted births, and we calculate the mean age at childbearing (MAB) using both counts and rates.
```{r}
BEstats <- 
  BE %>% 
  group_by(country, year) %>% 
  
  # TFR is a simple sum since we have single-age data
  summarize(tfr = sum(fx),
            tfr_no_scale = sum(births / expos),
            
            # MAB is a weighted average
            mab_counts = sum((age + .5) * births_hat) / sum(births_hat),
            mab_fx = sum((age+5) * fx) / sum(fx),
            .groups = "drop")
```

## NEW: visualize the results:

### ASFR curves
Again, this is just for priming! 
```{r, warning = FALSE}
BE %>% 
  ggplot(aes(x = age, y = fx, color = country, alpha = year)) +
  geom_line() +
  theme_minimal()
```

### TFR

```{r}
BEstats %>% 
  ggplot(aes(x = year, y = tfr, color = country, group = country)) +
  geom_line() +
  theme_minimal()
```

### MAB

It appears to make a difference what we use as weights. This isn't just a general level shift: rankings and trends can also change!
```{r}
BEstats %>% 
  # we want to stack the two mab variables
  select(country, year, mab_counts, mab_fx) %>% 
  pivot_longer(c(mab_counts, mab_fx),
               names_to = "type",
               values_to = "mab") %>% 
  
  # that way we can *map* the type of MAB to linetype...
  ggplot(aes(x = year, 
             y = mab, 
             color = country, 
             linetype = type, 
             group = interaction(country, type))) +
  geom_line() +
  theme_minimal()
```

Note, comparing the TFR and MAB trends, and with some to still-to-acquire intuition, we may surmise that the downward TFR trends for Croatia and Belgium might be offset somewhat if we do the Bongaarts-Feeney adjustment proposed in the handout exercise. We'll only get results for the central 3 years though...

## An adjustment beyond the intended scope of this workshop

### Summary
It was noticed by a participant (front row, sorry I didn't catch your name yet! Will update here when I find out) that the sum of births with stated age plus unknowns is less than the stated total, and we realized that this is likely due to data truncation below age 15 and over age 49. That means we have some births where they knew the mothers' age, but it is simply not included because they only give ages 15-49. First, before you read this, you should understand that anything beyond this point is not for the sake of a better TFR estimate-- it will barely budge. We add this rigor in case, in some unforeseen application, the full age range of ASFR is of interest.

Let's first confirm this suspicion before considering doing anything about it.

Observe, if the count in age 15 is actually "age 15 and under", and if age 49 is actually age "49+", then we should be able to detect this visually. Better in the rates than in the counts. Let's take a zoomed-in look:

```{r}
BE %>% 
  filter(age < 19) %>% 
  ggplot(aes(x = age, y = fx, group = interaction(country, year))) +
  geom_line() +
  scale_y_log10() +
  labs(title = "I see no evidence that age 15 is actually the sum of ages 10-15.",
       subtitle = "It's literally just age 15, so we're missing births below")
```
```{r}
BE %>% 
  filter(age > 46) %>% 
  ggplot(aes(x = age, y = fx, group = interaction(country, year))) +
  geom_line() +
  scale_y_log10() +
  labs(title = "Probably also the same story in age 49.",
       subtitle = "I see only one line that goes down and then up")
```

So, we conclude that truncation is probably at work. It turns out the best way forward will be to model the tails on the rate scale, then convert back to counts, and rescale *only the tails* so that they sum to the remainder (total - (births ages 15-49 + unknown). On the other hand, one *could* reasonably suppose that any births of unknown maternal age are very likely not coming from ages outside the normal range. Ages outside the normal range might be more investigated (low end) or more assisted (higher end), and so likely be of known age. If that were the case, then we would want to redistribute unknowns over the observed age distribution rather than rescaling it to the total, and in that case the solution would look just like what we did for population counts, *except* we know that any resulting TFR would be an underestimate, because the tails are non-zero. It's worth reiterating that we only care about extrapolating to cover higher and lower ages if we care about the *entirety* of the fertile age range. We do not care to do this extrapolation if we care about the tails and the tails only. In that case you'd seek an alternate source of information without age truncation.

### Worked solution

To handle the tails and overall rescaling in a more rigorous fashion is, to put it lightly, a bit more involved. The code below gets the job done in one of the many possible ways. This is probably how I would do it myself if I found myself in exactly this situation and needed to publish an official report, or similar. In that situation, realistically, we'd have access to the full age range. So this exercise is somewhat contrived, but it serves, if you follow the logic, to demonstrate assumptions commonly applied in the field.

First, we should size up the discrepancy
```{r}
# save this bit, since we need to handle the part afterwards in different ways
B_in <-  read_excel(path = "Data/demo_fasec.xlsx",
             range = "A10:H158",
             na = ":") %>% 
  pivot_longer(cols = `2011`:`2016`,
               names_to = "year",
               values_to = "births") %>% 
  rename(age = AGE,
         country =`GEO/TIME`)

# save totals, which we can re-join later for final scale
B_totals <- 
  B_in %>% 
  filter(age == "Total") %>% 
  rename(total = births) %>% 
  select(-age)
```
Go ahead and View this object to get a sense of it. We save this object for instrumental use later.

Now we infer / calculate the tail totals. That is, how many births had a known maternal age but were truncated? We save this object for instrumental use later.
```{r}
# infer size of "known-age" component of tails
B_tail_totals <-
  B_in %>% 
  group_by(country, year) %>% 
  summarize(total = births[age == "Total"],
            marginal_total = sum(births[age != "Total"]),
            .groups = "drop") %>% 
  mutate(both_tails = total - marginal_total) %>% 
  select(country, year, both_tails)
```

Now, we opt to infer the tail *rate* distribution by applying a graduation method developed by Carl Schmertmann, which you can find documented here: [https://schmert.net/calibrated-spline/REBEP/](https://schmert.net/calibrated-spline/REBEP/). Demographers usually extrapolate things on the rate scale because rates are a mirror to forces that behave in (ideally) regular ways. Counts on the other hand are part due to rates and part due to population structure. The graduation method we'll apply is premised on rates in 5-year age groups. It works really well in my experience. I've taken a version of Prof Schmertmann's code and modified / simplified it for our case. This was only possible because his original code was fully reproducible. The plan is to aggregate births and exposures to 5-year age groups, then calculate rates, then graduate using the Schmertmann method, which will give back rates single ages 12-54. 

`R` note: it would be cool to do `group_by()` then `mutate()` to move the 5-year data to single ages, but `mutate()` requires us to keep the same rows, so that won't work. `group_modify()`, can handle this. To apply it, our function (we see how to write them Monday!) needs to take a `tibble` and return a `tibble`, so it's written in that way. The objects `K8` and `S` are instrumental to the method used. 
```{r}

# extrapolate rates to tails
# we'll treat this as a rate graduation problem, just for the sake
# of the tails. Will require a check on plausibility.

source("https://raw.githubusercontent.com/timriffe/KOSTAT_Workshop1/master/03_fertility_ad_hoc.R")

B_infer_cs <-
  BE %>% 
  select(country, year, age, births, expos) %>% 
  
  # create 5-year ages
  mutate(age = age - age %% 5) %>% 
  arrange(country, year, age) %>% 
  group_by(country, year, age) %>% 
  summarize(births = sum(births),
            expos = sum(expos),
            .groups = "drop") %>% 
  
  # get rates and then graduate
  mutate(asfr = births / expos) %>% 
  group_by(country, year) %>% 
  group_modify(~graduate_CS_df(data = .x,S=S,K8=K8)) %>% 
  ungroup()
```
The above result `B_infer_cs` just has rates, however, so we need to rejoin to `E` to get exposures for all ages 12-54. After joining we can convert tail rates back to counts, then only keep ages 12-14 and 50-54, then join our previously calculate `tail_totals` and do our first-pass rescale of the tails.
```{r}
# convert rate extrapolation in tails to births,
# then rescale to previously inferred tail totals,
# save only the tail ages (<15 and >= 50)
B_tails <-
  # match exposures to the newly estimated rates
  E %>% 
  select(year, country, age, expos) %>% 
  inner_join(B_infer_cs, by = c("country","year","age")) %>% 
  
  # get implied births
  mutate(births_cs = asfr * expos) %>% 
  select(-asfr) %>% 
  
  # cut to tail ages
  filter(age < 15 | age >= 50) %>% 
  
  # append tail totals and rescale
  left_join(B_tail_totals, by = c("country","year")) %>% 
  arrange(country, year, age) %>% 
  group_by(country, year) %>% 
  mutate(births_cs_scaled = births_cs / sum(births_cs) * both_tails) %>% 
  
  # mimic column names of original births so we can bind the rows
  select(country, year, age, births = births_cs_scaled)
```
`B_tails` contains our initial estimate of births in single ages in the tails. We append this to the original births using `bind_rows()`, then re-join exposures, then join our stated totals, and finally calculate ASFR.

```{r}
# stack with births aged 15-49, and
# *now* we can rescale to total
B_final <-
  # append new tails
  B %>% 
  select(-total,-births_hat) %>% 
  bind_rows(B_tails) %>% 
  arrange(country, year, age) %>% 
  filter(year > 2011) %>% 
  
  # re-join exposure
  inner_join(E, by = c("age", "country", "year")) %>% 
  select(country, year, age, births, expos) %>% 
  
  # add country-year total births
  left_join(B_totals, by = c("country","year")) %>% 
  
  # rescale (all ages 12-54)
  group_by(country, year) %>% 
  mutate(births = births / sum(births) * total) %>% 
  select(-total) %>% 
  
  # final asfr estimate
  mutate(asfr = births / expos,
         version = "final")
```

Now we'd like to combine these with our original ASFR estimates from the session, which we had stored in an object `BE`. THis we do by stacking because it eases visual comparisons with `ggplot()`, we just need to differentiate them with `version` (`"sesion"` or `"final"`).
```{r}
# combine the two versions
BE_compare <-
  BE %>% 
  select(country, year, age, births = births_hat, expos, asfr = fx) %>% 
  mutate(version = "session") %>% 
  bind_rows(B_final) 
```

Now let's recalculate summary statistics for both versions, so we can see how much difference all this made
```{r}
# compare summary statistics
B_stats_compare <-
  BE_compare %>% 
  group_by(country, year, version) %>% 
  summarize(tfr = sum(asfr),
            mab_rates = sum((age+.5) * asfr) / sum(asfr),
            mab_counts = sum((age + .5) * births) / sum(births),
            total = sum(births),
            .groups = "drop")
```

It turns out, none of this work was for the sake of better summary statistics! We just got a more complete age range. Any value perturbations are *within* the normal stochastic range, so this is a trivial adjustment. Really, I'd still do it for the sake of a full age range that ends with 0s on the left and right. If unknowns were very high, or if tail values were comparatively high, then this exercise would be less innocuous, and some more decisions would need to be made.
```{r}
# observe, this was not for the sake of summary stats!
B_stats_compare %>% 
  select(country, year, tfr, version) %>% 
  pivot_wider(names_from = version, values_from = tfr)

B_stats_compare %>% 
  select(country, year, mab_counts, version) %>% 
  pivot_wider(names_from = version, values_from = mab_counts)
```

Save a copy for the exercises

```{r}
B_final %>% 
  write_csv("Data/03_results.csv")
```

*Fin*
