---
title: | 
  | \includegraphics{logotip.pdf}
  |
  | KOSTAT-UNFPA Summer Seminar on Population
  | \vspace{1.5cm} \LARGE \emph{Workshop~1.~Demography in R}
  | \vspace{0.3cm} \huge \textbf{Day 2: The tidy data approach}\vspace{0.6cm}
  | 
fontsize: 11pt
geometry: a4paper, twoside, left=2.5cm, right=2.5cm, top=2cm, bottom=2.8cm, headsep
  = 1.35cm, footskip = 1.6cm
output:
  pdf_document:
    number_sections: yes
  html_document2: default
  html_document:
    number_sections: yes
    toc: yes
  pdf_document2: default
  header-includes:
    - \usepackage{titling}
    - \usepackage{fancyhdr}
    - \pagestyle{fancy}
    - \fancyhead[LE]{\thepage~\qquad~KOSTAT-UNFPA Summer Seminar on Population}
    - \fancyhead[RE]{Workshop~1.~Demography in R}
    - \fancyhead[LO]{{Day 2: The tidy data approach}}
    - \fancyhead[RO]{Tim Riffe\qquad~\thepage}
bibliography: bibliography.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\noindent\makebox[\textwidth][c]{
  \begin{minipage}[t]{0.8\textwidth}
    \centering
    \Large{Instructor: Tim Riffe \\ \texttt{tim.riffe@ehu.eus}}
   
    \vspace{.5cm}
    \Large{Assistants: \\ Jinyeon Jo: \texttt{jyjo43043@gmail.com} \\ Rustam Tursun-Zade: \texttt{rustam.tursunzade@gmail.com}}
  \end{minipage}
}


\vspace{0.8cm}
\begin{center}
\large{28 July 2022}
\end{center}
\vspace{0.8cm}


\tableofcontents

# Tidy data

## Definition
Tidy data follows a standard structure where each column is a variable, each row is an observation, and each cell is a value. Anything else is messy. It's literally that straightforward. A more complete definition can be found here: [https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html) Demographic data is often delivered in a tidy format. When it is not, then it can be reshaped into a tidy format.

Tidyverse packages work well together because they share a standard approach to formatting and working with datasets. Tidy datasets processed using tidyverse tools allow for fast and understandable analyses that in many cases require no *programming*, whereas it often takes a certain amount of head-scratching (programming) to analyze not-tidy datasets. 

Tidy datasets can also be visualized without further ado using a systematic *grammar* [@wilkinson2012grammar] implemented in the `ggplot2` package ( @wickham2016ggplot2, this loads automatically with `tidyverse`). Today we will do just basic examples, but this will be made more explicit as the workshop progresses.

## Example (gapminder)

The so-called `gapminder` dataset is an example of *tidy* data that allows to demonstrate some of the basic `tidyverse` concepts. Let's install this package and have a look. Remember to comment out the installation line of code using `#` after you install it once!

```{r, eval = FALSE, message = FALSE}
install.packages("gapminder")
```

```{r}
library(gapminder)
library(tidyverse)
?gapminder
#View(gapminder)
# list the data structure:
str(gapminder)
```
In this data, unique combinations of country and year are what define an observation. From the above call to `str()` we see the structure of the data, which indicates the column types and the number of rows (1704). We therefore have 1704 observations. `continent` is a property of `country` here, and is not a structural variable.

We have three *variables* spread over the columns, life expectancy at birth `lifeExp`, population size `pop`, and GDP per capita `gdpPercap`. 

## Basic dataset descriptives

There is a function called `summary()` that guesses how we would like the data summarized:

```{r}
summary(gapminder)
```

The result tells us that there are 12 observations for each country, that there are 624 observations in Africa, 300 in the Americas, etc, and it usefully gives the range and quartiles of each variable. For example life expectancy observations in the data range from 23.6 to 82.6. Wow!

One can also query specific columns like so: We can check the year range like so:

```{r}
unique(gapminder$year)
# or
gapminder %>% pull(year) %>% unique()
```
This data is in 5-year intervals, and year values appear to be approximately centered within standard intervals. i.e. 1950-1954 gets the value 1952. We have about 50 years of history here. We give two equivalent ways of asking this question of the data. The second is easier to deparse visually, even if we've not yet introduced the operator `%>%` or the function `pull()`. It reads "take the gapminder data, then pull off the year column, then give the unique values". The symbol `%>%` reads as "then". The expression `unique(gapminder$year)` on the other hand is somehow inverted, meaning that it reads from the inside out. We start with the year column of gapminder, then look outward to see that we extract its unique values. Both are valid approaches. The one using pipes is the version we will more often attempt to use in this workshop. 

### Mini exercise 

List which countries are in the data, and write how many there are:
```{r}
# 
```


## pipes

The pipe operator, more explicitly works by evaluating an object on the left and sending the result to the function on the right.

For example, the below pipe separates step 1 (the drawing of 10 random deviates of the uniform distribution) from step 2 (calculation of their mean).
```{r}
runif(10) %>% mean()
```

One can chain together a sequence of operations like so:

```{r}
runif(10) %>% 
  sort() %>% 
  cumsum()
```
This code reads in order "take ten random uniform draws, then sort them (in ascending order), then calculate their cumulative sum". Let's call this sort of code statement a *pipeline*, since it defines a multistep sequence of execution steps. We will be construction data analysis sequences using this trick for the entirety of the workshop. If it is not immediately clear what is happening here, do not worry, it will make sense as we progress through the material, and I will redundantly narrate each code chunk multiple times.

### Mini Exercise for pipes 

Take 100 random draws of the Poisson distribution, with lambda parameter equal to 100 (`rpois()`), and calculate the 95% prediction interval using `quantile(x, probs = c(.025,.975))`. Note that the argument `x` is simply going to be the incoming data from `rpois()`, and you don't need to specify the argument `x` at all.
```{r}
# 
```

I introduce this now, so that we may use it naturally in what comes.

## filtering is for rows

Filtering in the `tidyverse` implies the potential deletion of rows based on some logical criteria. Observe:

```{r}
A <- tibble(a = 0:10, 
            b = letters[1:11])
A
A %>% 
  filter(a > 5)

# rows where 5 divides evenly into `a`
A %>% filter(a %% 5 == 0)

# just a particular case
A %>% 
  filter(b == "c")

# a vector of cases:
A %>% 
  filter(b %in% c("b", "f", "g"))
```

As you can see, logical evaluation is the key to making intelligent use of `filter()`. You can query columns in the data directly within the filter call. The key is to produce a value of either `TRUE` or `FALSE` for each row of the data. Where the logical expression evaluates to `TRUE` we keep the rows, and `FALSE`s are discarded. Some useful logical operators include 
1. `==` test equality
2. `>=` (`<=`) test inclusive greater than (less than)
3. `%in%` test membership
4. `any()` is any element in a vector `TRUE`
5. `all()` are all elements of a vector `TRUE`
6. `!` negation of any of the above
7. `between()` tests if a value is in an interval
8. `&` logical AND
9. `|` logical OR

More examples:
```{r}
A %>% 
  # between() is by default inclusive in its bounds
  filter(between(a, 3, 5) | b == "g")

A %>% 
  # multiple conditions
  filter(a < 7,
         a >= 2,
         b %in% c("a","c","e","g","i","k"))
```

Note `filter()` accepts comma-separated arguments, interpreting the commas as `&`.

### Mini Exercises for filters

1. How many rows of `gapminder` have a life expectancy between 50 and 60, inclusive

2. Which countries have ever had a life expectancy greater than 78?

## selecting is for columns

Sometime we don't need all the columns in the data. We can select particular ones by name or position, like so:

```{r}
gapminder %>% 
  select(country, year, gdpPercap)

gapminder %>% 
  select(1,2,6)
```

You could also select columns based on various kinds of column name string matching, like so

```{r}
gapminder %>% 
  select(contains("p"))

# or
gapminder %>% 
  select(ends_with("p"))
```
Some other useful options are `starts_with()`, and so on. The functions `contains()`, `starts_with()` and `ends_with()` are selecting columns using logical expressions, just as we've done for filtering rows. Try typing `?starts_with` into the console, and pick the help file from the `tidyselect` package to see a listing of other helper functions for column selection.

## create columns with `mutate()`

To create a column, potentially using other columns you already have, use `mutate()` like so:

```{r}
gapminder %>% 
  mutate(GDP = pop * gdpPercap)
```

You can create several columns at once, by separating with commas, and columns can be sequentially dependent. Observe:

```{r}
gapminder_hypothetical <-
  gapminder %>% 
  mutate(GDP = pop * gdpPercap,
         GDP100 = gdpPercap * 100,
         stationary_births = pop / lifeExp,
         GDP_alternative = stationary_births * GDP100,
         GDP_ratio = GDP_alternative / GDP) 
```

Note that `GDP`, `GDP100`, `stationary_births`, and `GDP_alternative` are created, and each used later as well within the same `mutate()` call. Whenever it makes sense in your calculations, it's nice to include such operations in a single `mutate()` call rather than in a series of `mutate()` calls. It's cleaner that way. These calculations are meant to showcase `mutate()` usability, not 

Aside: What are those strange measures? They're purely hypothetical and likely not useful extrapolations of the data points give, making strange invocations of a stationary world. `GDP` is a direct calculation, not controversial. `GDP100` is calculated on the assumption that if a newborn were to accumulate on average `gdpPercap` of income per year over 100 years, how much would the lifetime `gdpPercap` be? `stationary_births` makes use of the stationary relationship $b = N / e(0)$, meaning that the stationary crude birth rate $b$ is the population size `N` divided by life expectancy. For us `N` is `pop`, but of course neither the population size/structure nor period life expectancy are actually stationary. So this quantity is not usable per se. We then create `GDP_alternative`, which scales `GDP100` to a hypothetical cohort size and says "what would be the lifetime GDP in this fake longevous stationary population?". Finally, we take the ratio. It seems lengthening life to a consistent 100 years would (under these unrealistic constraints) increase GDP.

What does the result look like? (plot code discussed later in workshop)
```{r}
gapminder_hypothetical %>% 
  ggplot(aes(x = year, y = GDP_ratio, group = country)) +
  geom_line() +
  scale_y_log10() +
  geom_hline(yintercept = 1, color = "red")
```

Remember that since `mutate()` just creates columns, it does not change the number of rows in the data.

### Mini exercise

1. Take 2 minutes to write down as many problems as you can think of in the above analysis, no coding required. 

## Create aggregate measures using `summarize()`

You can create aggregate measures using `summarize()`, implying a likely reduction in the number of rows in the data. For example, we can calculate the total GDP for the most recent year in the data as follows:

```{r}
gapminder %>% 
  filter(year == max(year)) %>% 
  summarize(GDP = sum(pop * gdpPercap))
```

### Mini Exercise for `summarize()`

1. Calculate the average life expectancy over all countries in the most recent year.
2. Calculate the population weighted average of life expectancy in the most recent year.

The formula for a weighted average is:

$$ \bar{x} = \frac{\sum x_i \cdot w_i}{\sum w_i}$$

where $x$ is the thing being weighted and $w$ are the weights. We use this formula in many many places in demography!
```{r}
#
```

## Use `group_by()` to scale up!

The value of `mutate()` and `summarize()` increases greatly if we learn to make intelligent use of these constructs for subgroups in the data. What if we want to pick out the highest life expectancy per year in the data? To get the highest life expectancy in the data we do:

```{r}
gapminder %>% 
  filter(lifeExp == max(lifeExp))
```
To do this for each year, use `group_by()` to impose independent groups in the data on the basis of variables, then do everything else just the same:

```{r}
gapminder %>% 
  # apply independent groups
  group_by(year) %>% 
  # whatever we do here is independent within groups!
  filter(lifeExp == max(lifeExp)) %>% 
  # remove when done
  ungroup() %>% 
  # sort
  arrange(year)
```

Note, we should remove groups with `ungroup()` when we're done using them, and we sort the data on year for visual inspection using `arrange()`. This reads as "first take `gapminder`, then `group_by()` year, then `filter()` out the highest life expectancy per year, then remove the groups and sort years". Notice how the functions can be read as verbs, and the the pipes allow them to be combined into a rote kind of sentence. Indeed, it can help to add notes using `#`: don't worry: it won't break the chain! As the dataset goes down the pipeline, by default it becomes the first argument to the next function to be executed. Each of these functions has a first argument called either `data` or `.data`, which doesn't need to be specified because the incoming data is passed to it. 

Note: you can run this code by simply placing the cursor anywhere in the pipeline and pressing `Ctrl + Enter`. There is no need to select the whole statement before running, although this also works (you could in this case also click the green play arrow).

### Exercises for `group_by()`

1. Aggregate all variables by continent and year. For `lifeExp` and `gdpPercap`, use the population weighted averages, as we did above. Is this a job for `mutate()` or `summarize()`?

2. Calculate year-on-year life expectancy changes for each country. Tip, use `lead()` or `lag()`, like so:

```{r}
# a series incrementing in steps of 1.
a <- c(1,3,4,5,7,10)
lead(a) - a
# *or*
a - lag(a)
```
Note, the first or last year will get `NA`s. You should ensure that years are sorted within countries using `arrange(country, year)`. Is this a job for `mutate()` or `summarize()`? I personally would use the `lead()` version of this approach.

## reshape using `pivot_wider()` and `pivot_longer()`

The `gapminder` data is already `tidy`, and most often we use long / wide reshaping operations to force non-tidy data into a tidy form. A wide version of `gapminder` might, for example, have years spread over columns rather than stacked. Sometimes government statistical offices distribute data like so. Here `names_from` is the column whose values will determine the new column names, whereas `values_from` is where data will be drawn from.

```{r}
gapminder_wide <-
  gapminder %>% 
  select(country, year, continent, pop) %>% 
  pivot_wider(names_from = year,
              values_from = pop)
```
At times, we actually want our data to look like this for some ad hoc calculation convenience. Note, you can list more than one `values_from` column. We could spread each variable-year combinations by specifying them in a vector. In this case, newly created column names will be concatenated.

```{r}
gapminder %>% 
  pivot_wider(names_from = year,
              values_from = c(lifeExp, gdpPercap, pop))
```

To go in the other direction we use `pivot_longer()`. This is the more commonly used of the two in practice. Here we specify a column range, then give the names as strings for the columns that will collect the header names and the values. In this case, we give the columns by name, but since they're numbers, we put the names in back tics to ensure they will be interpreted as names rather than as positions!

```{r}
gapminder_wide %>% 
  pivot_longer(`1952`:`2007`,
               names_to = "year",
               values_to = "pop")
```
That's fine if you happen to know the column names and if they happen to be in a continuous range, as we have here. You could also use position like so (if you know them)

```{r}
gapminder_wide %>% 
  pivot_longer(3:14,
              names_to = "year",
              values_to = "pop")
```

Or we could exploit the numeric nature of the column names and use one of the `tidyselect` functions we saw before.
```{r}
gapminder_wide %>% 
  pivot_longer(num_range(prefix = "", range = 1952:2007),
               names_to = "year",
               values_to = "pop")
```

Note that `pivot_longer()` also accepts multiple `names_to` destinations, if you happen to have concatenated names, as in the complex `pivot_wider()` example. Here we make and then undo the ugly case above with concatenated variable and year column names. This needs to be done in two steps: first we store the full column range (`3:ncol(.)`, where `ncol(.)` gives the number of columns of the incoming data), splitting names into two new columns for variables and years, and putting all variables together. At this stage the data is *too* long to be easily usable, and we still need to spread variables over the columns, so the final `pivot_wider()` statement does this for us.

```{r}
gapminder %>% 
  pivot_wider(names_from = year,
              values_from = c(lifeExp, pop, gdpPercap)) %>% 
  # two steps: 1) collect it all
  pivot_longer(3:ncol(.),
               values_to = "value",
               names_to = c("variable","year"),
               names_sep = "_") %>% 
  # 2) spread variables back out
  pivot_wider(names_from = variable,
              values_from = value)
```

### Mini exercises

1. Take the `gapminder` data, remove the `continent` column, then spread `country` over columns using `pivot_wider()`.

2. Undo the result of part 1 of this exercise to return to the original `gapminder` data.


# Looking ahead

Tomorrow we will do a longer worked example using these basic `tidyverse` operations and introducing some other recoding and dataset joining operations, on the example of fertility data.


